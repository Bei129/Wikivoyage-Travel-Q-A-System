{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP 631 Project Task 2\n",
        "Fei Teng (ft28), Lingyi Xu (lx28)\n",
        "\n",
        "March 23rd, 2025"
      ],
      "metadata": {
        "id": "yjb8KDI_cFUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary package for dunzhang/stella_en_400M_v5\n",
        "!pip install xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "215UEchp46WM",
        "outputId": "18b85391-3ad9-44f2-a16b-1e74e69de0b5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from xformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Precomputed Index\n",
        "This part loads a large corpus from a JSON file using stream parsing (via ijson) to avoid excessive memory usage. It uses the StellaEmbedding class to encode document texts (concatenated title and text) into fixed-length embeddings. The documents are processed in small chunks (e.g., 1000 documents per chunk) and the resulting embeddings, along with their document IDs, are aggregated and saved to disk as a precomputed index. This precomputation step enables fast future retrieval without re-encoding the entire corpus."
      ],
      "metadata": {
        "id": "lgP5XsDQQn5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import ijson\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "class StellaEmbedding:\n",
        "    def __init__(self, model_name=\"dunzhang/stella_en_400M_v5\", device=\"cuda\", max_length=256, use_half=False):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        if use_half:\n",
        "            self.model.half()\n",
        "        self.model.eval()\n",
        "        self.max_length = max_length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_batch(self, texts, batch_size=8):\n",
        "        all_embs = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            encoded = self.tokenizer(batch, padding=True, truncation=True,\n",
        "                                       max_length=self.max_length, return_tensors='pt')\n",
        "            for k in encoded:\n",
        "                encoded[k] = encoded[k].to(self.device)\n",
        "            outputs = self.model(**encoded)\n",
        "            token_embeddings = outputs.last_hidden_state  # [B, seq_len, hidden_dim]\n",
        "            attention_mask = encoded['attention_mask'].unsqueeze(-1).float()  # [B, seq_len, 1]\n",
        "            sum_emb = torch.sum(token_embeddings * attention_mask, dim=1)\n",
        "            denom = torch.clamp(attention_mask.sum(dim=1), min=1e-9)\n",
        "            embs = sum_emb / denom\n",
        "            all_embs.append(embs.cpu())\n",
        "        return torch.cat(all_embs, dim=0)\n",
        "\n",
        "def stream_corpus(json_path, chunk_size=1000):\n",
        "    \"\"\"\n",
        "    Use ijson to stream parse corpus JSON files, returning a list of chunk_size individual (doc_id, doc) tuples at a time.\n",
        "    JSON structure ：{ doc_id1: { \"title\": ..., \"text\": ...}, doc_id2: { ... }, ... }\n",
        "    \"\"\"\n",
        "    batch = []\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        for doc_id, doc in ijson.kvitems(f, ''):\n",
        "            batch.append((doc_id, doc))\n",
        "            if len(batch) >= chunk_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        if batch:\n",
        "            yield batch\n",
        "\n",
        "def build_index(corpus_path, index_path=\"precomputed_index.pt\", chunk_size=1000, batch_size=8):\n",
        "    embedder = StellaEmbedding(model_name=\"dunzhang/stella_en_400M_v5\", device=\"cuda\", max_length=256, use_half=False)\n",
        "    all_doc_ids = []\n",
        "    embeddings_list = []\n",
        "    total_docs = 0\n",
        "\n",
        "    for batch in stream_corpus(corpus_path, chunk_size=chunk_size):\n",
        "        doc_ids = [doc_id for doc_id, _ in batch]\n",
        "        texts = []\n",
        "        for doc_id, doc in batch:\n",
        "            title = doc.get(\"title\", \"\")\n",
        "            text = doc.get(\"text\", \"\")\n",
        "            texts.append(title + \" \" + text)\n",
        "\n",
        "        batch_embs = embedder.encode_batch(texts, batch_size=batch_size)\n",
        "        embeddings_list.append(batch_embs)\n",
        "        all_doc_ids.extend(doc_ids)\n",
        "        total_docs += len(batch)\n",
        "        print(f\"Processed {total_docs} documents\")\n",
        "        del doc_ids, texts, batch_embs\n",
        "        gc.collect()\n",
        "\n",
        "    # Merge vectors of all blocks\n",
        "    all_embeddings = torch.cat(embeddings_list, dim=0)  # shape: [N, hidden_dim]\n",
        "    print(f\"Total embeddings shape: {all_embeddings.shape}\")\n",
        "    # Saving precomputed indexes to disk\n",
        "    torch.save({\"doc_ids\": all_doc_ids, \"embeddings\": all_embeddings}, index_path)\n",
        "    print(f\"Precomputed index saved to {index_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    corpus_path = \"/content/drive/MyDrive/corpus_dict.json\"\n",
        "    output_index_path = \"precomputed_index.pt\"\n",
        "    build_index(corpus_path, index_path=output_index_path, chunk_size=1000, batch_size=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL7T7Qe_Qlxo",
        "outputId": "d475e9e8-288d-40ae-ee36-bd31f29fbf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: {'new.pooler.dense.weight', 'new.pooler.dense.bias'}\n",
            "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000 documents\n",
            "Processed 2000 documents\n",
            "Processed 3000 documents\n",
            "Processed 4000 documents\n",
            "Processed 5000 documents\n",
            "Processed 6000 documents\n",
            "Processed 7000 documents\n",
            "Processed 8000 documents\n",
            "Processed 9000 documents\n",
            "Processed 10000 documents\n",
            "Processed 11000 documents\n",
            "Processed 12000 documents\n",
            "Processed 13000 documents\n",
            "Processed 14000 documents\n",
            "Processed 15000 documents\n",
            "Processed 16000 documents\n",
            "Processed 17000 documents\n",
            "Processed 18000 documents\n",
            "Processed 19000 documents\n",
            "Processed 20000 documents\n",
            "Processed 21000 documents\n",
            "Processed 22000 documents\n",
            "Processed 23000 documents\n",
            "Processed 24000 documents\n",
            "Processed 25000 documents\n",
            "Processed 26000 documents\n",
            "Processed 27000 documents\n",
            "Processed 28000 documents\n",
            "Processed 29000 documents\n",
            "Processed 30000 documents\n",
            "Processed 31000 documents\n",
            "Processed 32000 documents\n",
            "Processed 33000 documents\n",
            "Processed 34000 documents\n",
            "Processed 35000 documents\n",
            "Processed 36000 documents\n",
            "Processed 37000 documents\n",
            "Processed 38000 documents\n",
            "Processed 39000 documents\n",
            "Processed 40000 documents\n",
            "Processed 41000 documents\n",
            "Processed 42000 documents\n",
            "Processed 43000 documents\n",
            "Processed 44000 documents\n",
            "Processed 45000 documents\n",
            "Processed 46000 documents\n",
            "Processed 47000 documents\n",
            "Processed 48000 documents\n",
            "Processed 49000 documents\n",
            "Processed 50000 documents\n",
            "Processed 51000 documents\n",
            "Processed 52000 documents\n",
            "Processed 53000 documents\n",
            "Processed 54000 documents\n",
            "Processed 55000 documents\n",
            "Processed 56000 documents\n",
            "Processed 57000 documents\n",
            "Processed 58000 documents\n",
            "Processed 59000 documents\n",
            "Processed 60000 documents\n",
            "Processed 61000 documents\n",
            "Processed 62000 documents\n",
            "Processed 63000 documents\n",
            "Processed 64000 documents\n",
            "Processed 65000 documents\n",
            "Total embeddings shape: torch.Size([65000, 1024])\n",
            "Precomputed index saved to precomputed_index.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- 基于预计算索引的检索search_precomputed_index.py -->\n",
        "## Searching the Precomputed Index\n",
        "\n",
        "This part loads the precomputed index file, which contains the document IDs and their corresponding embeddings. For a given query string, it uses the same StellaEmbedding class to encode the query into an embedding. Then, it computes the cosine similarity between the query embedding and all document embeddings, and selects the top‑k most similar documents using torch.topk. The search function returns a list of dictionaries containing each document’s ID and its similarity score."
      ],
      "metadata": {
        "id": "75SN8wp-QsSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "class StellaEmbedding:\n",
        "    def __init__(self, model_name=\"dunzhang/stella_en_400M_v5\", device=\"cuda\", max_length=256, use_half=False):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        if use_half:\n",
        "            self.model.half()\n",
        "        self.model.eval()\n",
        "        self.max_length = max_length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_batch(self, texts, batch_size=8):\n",
        "        all_embs = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            encoded = self.tokenizer(batch, padding=True, truncation=True,\n",
        "                                       max_length=self.max_length, return_tensors='pt')\n",
        "            for k in encoded:\n",
        "                encoded[k] = encoded[k].to(self.device)\n",
        "            outputs = self.model(**encoded)\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "            attention_mask = encoded['attention_mask'].unsqueeze(-1).float()\n",
        "            sum_emb = torch.sum(token_embeddings * attention_mask, dim=1)\n",
        "            denom = torch.clamp(attention_mask.sum(dim=1), min=1e-9)\n",
        "            embs = sum_emb / denom\n",
        "            all_embs.append(embs.cpu())\n",
        "        return torch.cat(all_embs, dim=0)\n",
        "\n",
        "def cosine_scores(query_emb, doc_embs):\n",
        "    query_emb = query_emb.unsqueeze(0)  # [1, D]\n",
        "    scores = F.cosine_similarity(query_emb, doc_embs, dim=-1)\n",
        "    return scores\n",
        "\n",
        "def search_precomputed_index(index_path, query_text, top_k=5, batch_size=8):\n",
        "    # Load pre-calculated index\n",
        "    index_data = torch.load(index_path)\n",
        "    doc_ids = index_data[\"doc_ids\"]\n",
        "    embeddings = index_data[\"embeddings\"]  # shape: [N, hidden_dim]\n",
        "\n",
        "    # Code Search\n",
        "    embedder = StellaEmbedding(model_name=\"dunzhang/stella_en_400M_v5\", device=\"cuda\", max_length=256, use_half=False)\n",
        "    query_emb = embedder.encode_batch([query_text], batch_size=batch_size)[0]\n",
        "\n",
        "    # Calculate the cosine similarity of all documents to the query\n",
        "    scores = cosine_scores(query_emb, embeddings)\n",
        "\n",
        "    # Get top_k best results\n",
        "    top_scores, top_indices = torch.topk(scores, top_k, largest=True, sorted=True)\n",
        "    top_scores = top_scores.tolist()\n",
        "    top_indices = top_indices.tolist()\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(top_scores, top_indices):\n",
        "        results.append({\"doc_id\": doc_ids[idx], \"similarity\": score})\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    index_path = \"precomputed_index.pt\"\n",
        "    # sample question\n",
        "    query = \"What is a traditional breakfast in Mexico City?\"\n",
        "    top_k = 5\n",
        "    results = search_precomputed_index(index_path, query, top_k=top_k, batch_size=8)\n",
        "\n",
        "    print(\"\\nQ: \" + query)\n",
        "    print(\"Top-K results from precomputed index:\")\n",
        "    for rank, res in enumerate(results, start=1):\n",
        "        print(f\"{rank}. DocID={res['doc_id']}, similarity={res['similarity']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxO31qtjQx45",
        "outputId": "63adf3de-bc73-469d-9451-676700fad354"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: {'new.pooler.dense.weight', 'new.pooler.dense.bias'}\n",
            "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is a traditional breakfast in Mexico City?\n",
            "Top-K results from precomputed index:\n",
            "1. DocID=13444, similarity=0.6953\n",
            "2. DocID=35507, similarity=0.6940\n",
            "3. DocID=11626, similarity=0.6932\n",
            "4. DocID=35545, similarity=0.6927\n",
            "5. DocID=35517, similarity=0.6927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Document Previews\n",
        "This part loads a CSV file containing document preview information (document ID, title, and the first sentence) into a Python dictionary. This allows the retrieval system to later display additional information (such as the document title and a brief preview) alongside the search results. This approach avoids reloading the full corpus and keeps the interactive session lightweight."
      ],
      "metadata": {
        "id": "Um823E-UgT3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "index_path = \"precomputed_index.pt\"\n",
        "preview_csv = \"ids_titles_sentences.csv\"\n",
        "\n",
        "# Load CSV content into dictionary\n",
        "doc_previews = {}\n",
        "with open(preview_csv, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        doc_previews[row['id']] = {\n",
        "            'title': row['title'],\n",
        "            'first_sentence': row['first_sentence']\n",
        "        }\n",
        "\n",
        "# # Retrieve and display results\n",
        "# while True:\n",
        "#     query = \"What is a traditional breakfast in Mexico City?\"\n",
        "#     top_k = 5\n",
        "#     results = search_precomputed_index(index_path, query, top_k=top_k, batch_size=8)\n",
        "\n",
        "#     print(\"\\nQ:\", query)\n",
        "#     print(\"Top-K results from precomputed index:\")\n",
        "#     for rank, res in enumerate(results, start=1):\n",
        "#         doc_id = res['doc_id']\n",
        "#         similarity = res['similarity']\n",
        "#         title = doc_previews.get(doc_id, {}).get('title', 'N/A')\n",
        "#         first_sentence = doc_previews.get(doc_id, {}).get('first_sentence', 'N/A')\n",
        "#         print(f\"{rank}. Similarity={similarity:.4f}, DocID={doc_id}, Name={title}, Preview={first_sentence}\")\n",
        "#     break  # Add break here to prevent infinite loops"
      ],
      "metadata": {
        "id": "oxBXcrr2s5Pv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out warnings about unused weights during model initialization\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "hjE8mUjsbDpO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Search Session\n",
        "This interactive loop allows a user to repeatedly input queries and see the top‑k search results from the precomputed index. For each query, the system encodes the query, retrieves the most similar documents, and then displays for each result the document ID, similarity score, title, and a preview (first sentence). The session supports an exit command (typing \"exit\", \"quit\", or an empty input) so the user can terminate the interactive demo gracefully. All these steps use the precomputed index and the document preview dictionary, ensuring that the response time is very fast without needing to re-run lengthy model inference."
      ],
      "metadata": {
        "id": "urDc88nDgnzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Interactive Search Session (type 'exit' or leave empty to quit)\")\n",
        "while True:\n",
        "    query = input(\"Q: \").strip()\n",
        "    if query.lower() in [\"exit\", \"quit\", \"\"]:\n",
        "        print(\"Exiting interactive session.\")\n",
        "        break\n",
        "    top_k = 5\n",
        "    results = search_precomputed_index(index_path, query, top_k=top_k, batch_size=8)\n",
        "\n",
        "    print(\"\\nTop-K results from precomputed index:\")\n",
        "    for rank, res in enumerate(results, start=1):\n",
        "        doc_id = res['doc_id']\n",
        "        similarity = res['similarity']\n",
        "        title = doc_previews.get(doc_id, {}).get('title', 'N/A')\n",
        "        first_sentence = doc_previews.get(doc_id, {}).get('first_sentence', 'N/A')\n",
        "        print(f\"{rank}. Similarity={similarity:.4f}, DocID={doc_id}, Name={title}, Preview={first_sentence}\")\n",
        "    print(\"\\n\" + \"-\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv9E4unHwKZ3",
        "outputId": "4b2a0acc-9a83-4b13-f593-19790572a7b0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interactive Search Session (type 'exit' or leave empty to quit)\n",
            "Q: What are the top tourist attractions in Rome?\n",
            "\n",
            "Top-K results from precomputed index:\n",
            "1. Similarity=0.7515, DocID=2981, Name=Attractions, Preview=# AttractionsFor information on attractions at specific locations check out articles of nearby places or destinations further afield\n",
            "2. Similarity=0.7506, DocID=1781, Name=Ancient Rome tour, Preview=# Ancient Rome tourThe Forum Romanum tour is a walking tour around the Colosseo district in Rome, which was the centre of the Roman Empire\n",
            "3. Similarity=0.7466, DocID=46425, Name=ROM, Preview=# RomeFor other places with the same name, see Rome (disambiguation)\n",
            "4. Similarity=0.7444, DocID=46962, Name=Records, Preview=# AttractionsFor information on attractions at specific locations check out articles of nearby places or destinations further afield\n",
            "5. Similarity=0.7437, DocID=48010, Name=Rome, Preview=# RomeFor other places with the same name, see Rome (disambiguation)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Can you recommend a traditional breakfast in Mexico City?\n",
            "\n",
            "Top-K results from precomputed index:\n",
            "1. Similarity=0.6922, DocID=13444, Name=DF, Preview=# Mexico CityMexico City is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "2. Similarity=0.6882, DocID=35517, Name=Mexico City, Preview=# Mexico CityMexico City is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "3. Similarity=0.6882, DocID=35545, Name=Mexico city, Preview=# Mexico CityMexico City is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "4. Similarity=0.6862, DocID=11626, Name=City of Mexico, Preview=# Mexico CityMexico City is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "5. Similarity=0.6860, DocID=35505, Name=Mexican Federal District, Preview=# Mexico CityMexico City is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What are some popular local dishes to try in Thailand?\n",
            "\n",
            "Top-K results from precomputed index:\n",
            "1. Similarity=0.7591, DocID=57266, Name=Thai food, Preview=# Thai cuisineCuisines of Asia and OceaniaAustralian • Burmese • Cambodian • Central Asia • Chinese • Filipino • Indonesian • Japanese • Korean • Malaysian, Singaporean and Bruneian • Middle Eastern • South Asian • Thai • VietnameseThai cuisine is one of the most popular cuisines in the world because of its lightly prepared dishes with strong aromatic components and a spicy edge\n",
            "2. Similarity=0.7516, DocID=57263, Name=Thai cuisine, Preview=# Thai cuisineCuisines of Asia and OceaniaAustralian • Burmese • Cambodian • Central Asia • Chinese • Filipino • Indonesian • Japanese • Korean • Malaysian, Singaporean and Bruneian • Middle Eastern • South Asian • Thai • VietnameseThai cuisine is one of the most popular cuisines in the world because of its lightly prepared dishes with strong aromatic components and a spicy edge\n",
            "3. Similarity=0.6734, DocID=25832, Name=Japanese food, Preview=# Japanese cuisineCuisines of Asia and OceaniaAustralian • Burmese • Cambodian • Central Asia • Chinese • Filipino • Indonesian • Japanese • Korean • Malaysian, Singaporean and Bruneian • Middle Eastern • South Asian • Thai • VietnameseThe traditional cuisine of Japan (和食, washoku), renowned for its emphasis on simple, fresh, and seasonal ingredients, has taken the world by storm and been a UNESCO Intangible Cultural Heritage since 2013\n",
            "4. Similarity=0.6709, DocID=53181, Name=Songthaew, Preview=# ThailandThailand (Thai: ประเทศไทย Prathet Thai or เมืองไทย Mueang Thai) is the most visited country in Southeast Asia, and for good reason\n",
            "5. Similarity=0.6699, DocID=57273, Name=Thailand, Preview=# ThailandThailand (Thai: ประเทศไทย Prathet Thai or เมืองไทย Mueang Thai) is the most visited country in Southeast Asia, and for good reason\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How can I get around Paris using public transportation?\n",
            "\n",
            "Top-K results from precomputed index:\n",
            "1. Similarity=0.7480, DocID=45598, Name=Public transit, Preview=# Public transportationThis article is about local travel\n",
            "2. Similarity=0.7469, DocID=45608, Name=Public transport, Preview=# Public transportationThis article is about local travel\n",
            "3. Similarity=0.7468, DocID=45616, Name=Public transportation, Preview=# Public transportationThis article is about local travel\n",
            "4. Similarity=0.7442, DocID=35442, Name=Metro, Preview=# Public transportationThis article is about local travel\n",
            "5. Similarity=0.7433, DocID=46810, Name=Rapid transit, Preview=# Public transportationThis article is about local travel\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What cultural events should I attend in Kyoto this season?\n",
            "\n",
            "Top-K results from precomputed index:\n",
            "1. Similarity=0.7003, DocID=17447, Name=Events, Preview=# EventsMany cities have regular big cultural and sporting events, which draw tens of thousands of participants and lock up accommodation and transport; but it’s generally just for a day or so\n",
            "2. Similarity=0.6859, DocID=29650, Name=Ky%C5%8Dto, Preview=# KyotoKyoto is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "3. Similarity=0.6849, DocID=29648, Name=Ky%C5%8Dt%C5%8D, Preview=# KyotoKyoto is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "4. Similarity=0.6823, DocID=29613, Name=Kyoto, Preview=# KyotoKyoto is a huge city with several district articles that contain information about specific sights, restaurants, and accommodation\n",
            "5. Similarity=0.6634, DocID=29623, Name=East, Preview=# HigashiyamaGinkaku-jiSome of the most picturesque parts of Kyoto are located in Higashiyama (東山, lit\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Where can I find scenic hiking trails in the Swiss Alps?\n",
            "\n",
            "Top-K results from precomputed index:\n",
            "1. Similarity=0.7357, DocID=55610, Name=Swiss Alps, Preview=# Swiss AlpsThe Swiss Alps are in Switzerland, stretching over several regions\n",
            "2. Similarity=0.7106, DocID=5653, Name=Bernese Alps, Preview=# Bernese HighlandsThe southern end of canton Berne includes some of the tallest and most famous peaks in Switzerland including the Jungfrau and the Eiger, known as the Berner Oberland\n",
            "3. Similarity=0.7101, DocID=60682, Name=Vaud Alps, Preview=# Bernese HighlandsThe southern end of canton Berne includes some of the tallest and most famous peaks in Switzerland including the Jungfrau and the Eiger, known as the Berner Oberland\n",
            "4. Similarity=0.6955, DocID=55963, Name=S%C3%A4chsische Schweiz, Preview=# Saxon SwitzerlandNot to be confused with Switzerland, a country hundreds of miles to the southwest\n",
            "5. Similarity=0.6941, DocID=5652, Name=Bernese Highlands, Preview=# Bernese HighlandsThe southern end of canton Berne includes some of the tallest and most famous peaks in Switzerland including the Jungfrau and the Eiger, known as the Berner Oberland\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Q: \n",
            "Exiting interactive session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoWWwMIqTEyn",
        "outputId": "8c92d1c1-4af5-4007-f425-9493634d1bf8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}